wwj debug in scrapy spider init
---------------------baidu.com in queue
wwj debug return my_cachingThreadedResolver
******************************key---------------- None
wwj debug ------------ False
wwj debug------------- False
wwj debug ------------ False
wwj debug------------- False
write result to local pages
******************************key---------------- 220.181.112.244
wwj debug ------------ False
wwj debug------------- False
wwj debug ------------ False
wwj debug------------- False
write result to local pages
******************************key---------------- 220.181.112.244
wwj debug ------------ False
wwj debug------------- False
wwj debug ------------ False
wwj debug------------- False
write result to local pages
******************************key---------------- 220.181.112.244
wwj debug ------------ False
wwj debug------------- False
write result to local pages
******************************key---------------- 220.181.112.244
wwj debug ------------ False
wwj debug------------- False
write result to local pages
******************************key---------------- 220.181.112.244
wwj debug ------------ False
wwj debug------------- False
wwj debug ------------ False
wwj debug------------- False
wwj debug ------------ False
wwj debug------------- False
wwj debug ------------ False
wwj debug------------- False
write result to local pages
******************************key---------------- 220.181.112.244
wwj debug ------------ False
wwj debug------------- False
write result to local pages
wwj debug in spider start_request
wwj debug ------------ False
wwj debug------------- False
wwj debug ------------ False
wwj debug------------- False
wwj debug ------------ True
wwj debug------------- False
:13:49 [scrapy] INFO: Enabled item pipelines: ScrapyPipeline
2015-07-30 14:13:49 [scrapy] INFO: Spider opened
2015-07-30 14:13:51 [bloom_filter] DEBUG: Filtered duplicate request: <GET http://www.baidu.com/7.html>
2015-07-30 14:13:51 [bloom_filter] DEBUG: Filtered duplicate request: <GET http://www.baidu.com/8.html>
2015-07-30 14:13:51 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:13:51 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2015-07-30 14:13:51 [scrapy] DEBUG: Redirecting (302) to <GET http://www.baidu.com/search/error.html> from <GET http://www.baidu.com/6.html>
2015-07-30 14:13:51 [scrapy] INFO: wwj debug before  scheduler.enqueue_request
2015-07-30 14:13:51 [scrapy] DEBUG: Crawled (200) <GET http://www.baidu.com/search/error.html> (referer: None)
2015-07-30 14:13:52 [scrapy] ERROR: Spider error processing <GET http://www.baidu.com/search/error.html> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/usr/local/lib/python2.7/site-packages/scrapy/core/spidermw.py", line 48, in process_spider_input
    return scrape_func(response, request, spider)
  File "/usr/local/lib/python2.7/site-packages/scrapy/core/scraper.py", line 145, in call_spider
    dfd.addCallbacks(request.callback or spider.parse, request.errback)
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/internet/defer.py", line 299, in addCallbacks
    assert callable(callback)
AssertionError
Error during info_callback
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/usr/local/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1270, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/usr/local/lib/python2.7/site-packages/OpenSSL/SSL.py", line 926, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/usr/local/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1589, in get_app_data
    return self._app_data
  File "/usr/local/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1148, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-07-30 14:13:52 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/usr/local/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1270, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/usr/local/lib/python2.7/site-packages/OpenSSL/SSL.py", line 926, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/usr/local/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1589, in get_app_data
    return self._app_data
  File "/usr/local/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1148, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From cffi callback <function infoCallback at 0x7f9c45692b90>:
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/site-packages/OpenSSL/SSL.py", line 926, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/usr/local/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1589, in get_app_data
    return self._app_data
  File "/usr/local/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1148, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-07-30 14:13:52 [scrapy] DEBUG: Crawled (200) <GET https://www.baidu.com/s?wd=16&ie=utf-8&cl=3&t=12&fr=news> (referer: None)
2015-07-30 14:13:52 [scrapy] ERROR: Spider error processing <GET https://www.baidu.com/s?wd=16&ie=utf-8&cl=3&t=12&fr=news> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/usr/local/lib/python2.7/site-packages/scrapy/core/spidermw.py", line 48, in process_spider_input
    return scrape_func(response, request, spider)
  File "/usr/local/lib/python2.7/site-packages/scrapy/core/scraper.py", line 145, in call_spider
    dfd.addCallbacks(request.callback or spider.parse, request.errback)
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/internet/defer.py", line 299, in addCallbacks
    assert callable(callback)
AssertionError
2015-07-30 14:13:52 [scrapy] DEBUG: Crawled (200) <GET https://www.baidu.com/s?wd=15&ie=utf-8&cl=3&t=12&fr=news> (referer: None)
2015-07-30 14:13:52 [scrapy] DEBUG: Crawled (200) <GET https://www.baidu.com/s?wd=13&ie=utf-8&cl=3&t=12&fr=news> (referer: None)
2015-07-30 14:13:52 [scrapy] DEBUG: Crawled (200) <GET https://www.baidu.com/s?wd=12&ie=utf-8&cl=3&t=12&fr=news> (referer: None)
2015-07-30 14:13:52 [scrapy] ERROR: Spider error processing <GET https://www.baidu.com/s?wd=15&ie=utf-8&cl=3&t=12&fr=news> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/usr/local/lib/python2.7/site-packages/scrapy/core/spidermw.py", line 48, in process_spider_input
    return scrape_func(response, request, spider)
  File "/usr/local/lib/python2.7/site-packages/scrapy/core/scraper.py", line 145, in call_spider
    dfd.addCallbacks(request.callback or spider.parse, request.errback)
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/internet/defer.py", line 299, in addCallbacks
    assert callable(callback)
AssertionError
2015-07-30 14:13:52 [scrapy] ERROR: Spider error processing <GET https://www.baidu.com/s?wd=13&ie=utf-8&cl=3&t=12&fr=news> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/usr/local/lib/python2.7/site-packages/scrapy/core/spidermw.py", line 48, in process_spider_input
    return scrape_func(response, request, spider)
  File "/usr/local/lib/python2.7/site-packages/scrapy/core/scraper.py", line 145, in call_spider
    dfd.addCallbacks(request.callback or spider.parse, request.errback)
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/internet/defer.py", line 299, in addCallbacks
    assert callable(callback)
AssertionError
2015-07-30 14:13:52 [scrapy] ERROR: Spider error processing <GET https://www.baidu.com/s?wd=12&ie=utf-8&cl=3&t=12&fr=news> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/usr/local/lib/python2.7/site-packages/scrapy/core/spidermw.py", line 48, in process_spider_input
    return scrape_func(response, request, spider)
  File "/usr/local/lib/python2.7/site-packages/scrapy/core/scraper.py", line 145, in call_spider
    dfd.addCallbacks(request.callback or spider.parse, request.errback)
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/internet/defer.py", line 299, in addCallbacks
    assert callable(callback)
AssertionError
2015-07-30 14:13:52 [scrapy] DEBUG: Crawled (200) <GET https://www.baidu.com/s?wd=11&ie=utf-8&cl=3&t=12&fr=news> (referer: None)
2015-07-30 14:13:52 [scrapy] DEBUG: Crawled (200) <GET http://www.baidu.com> (referer: None)
2015-07-30 14:13:52 [scrapy] ERROR: Spider error processing <GET https://www.baidu.com/s?wd=11&ie=utf-8&cl=3&t=12&fr=news> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/usr/local/lib/python2.7/site-packages/scrapy/core/spidermw.py", line 48, in process_spider_input
    return scrape_func(response, request, spider)
  File "/usr/local/lib/python2.7/site-packages/scrapy/core/scraper.py", line 145, in call_spider
    dfd.addCallbacks(request.callback or spider.parse, request.errback)
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/internet/defer.py", line 299, in addCallbacks
    assert callable(callback)
AssertionError
2015-07-30 14:13:52 [scrapy] ERROR: Spider error processing <GET http://www.baidu.com> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/usr/local/lib/python2.7/site-packages/scrapy/core/spidermw.py", line 48, in process_spider_input
    return scrape_func(response, request, spider)
  File "/usr/local/lib/python2.7/site-packages/scrapy/core/scraper.py", line 145, in call_spider
    dfd.addCallbacks(request.callback or spider.parse, request.errback)
  File "/usr/local/lib/python2.7/site-packages/Twisted-15.2.1+r45258-py2.7-linux-x86_64.egg/twisted/internet/defer.py", line 299, in addCallbacks
    assert callable(callback)
AssertionError
2015-07-30 14:14:51 [scrapy] INFO: Crawled 7 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:15:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:16:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:17:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:18:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:19:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:20:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:21:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:22:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:23:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:24:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:25:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:26:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:27:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:28:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:29:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:30:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:31:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:32:51 [scrapy] INFO: Crawled 7 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-30 14:33:27 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force 
